{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RedConvolucional-FashionMnist.ipynb","provenance":[{"file_id":"1oPD1BP1PEAellhxWVmDzmZyx7v6-Z1oD","timestamp":1589840732547}],"collapsed_sections":[],"authorship_tag":"ABX9TyNLzWKz3wMQTKa8TJlBapnW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jpgJvmyKifV1","colab_type":"text"},"source":["#Deep Learning con Python\n","`Autor: Erwing FC \n","~erwingforerocastro@gmail.com`\n"]},{"cell_type":"markdown","metadata":{"id":"g2p5fHUCk5nO","colab_type":"text"},"source":["#Implementación de un modelo basico convolucional con keras \n","\n"]},{"cell_type":"code","metadata":{"id":"8gtJ1nXwlL1V","colab_type":"code","colab":{}},"source":["#importar librerias\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers \n","from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout, BatchNormalization,Flatten,Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pgBe0K1IqRjy","colab_type":"code","colab":{}},"source":["#preparamos loa datos\n","fashion_mnist = keras.datasets.fashion_mnist \n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() \n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] \n","\n","train_images = train_images.reshape((60000, 28, 28, 1)) \n","train_images = train_images.astype('float32') / 255 #convertimos y normalizamos\n","test_images = test_images.reshape((10000, 28, 28, 1)) \n","test_images = test_images.astype('float32') / 255\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekQ1qOS97nFM","colab_type":"code","colab":{}},"source":["#estructura del modelo\n","# model = Sequential() \n","# model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(28, 28, 1))) \n","# model.add(MaxPooling2D((2, 2)))                                          \n","# model.add(Conv2D(64, (5, 5), activation='relu'))                         \n","# model.add(MaxPooling2D((2, 2)))                                          \n","# model.add(Flatten())                                                      \n","# model.add(Dense(10,activation='softmax'))                                 \n","# model.summary()\n","\n","#agregamos el doble de neuronas\n","\n","# model = Sequential() \n","# model.add(Conv2D(64, (7, 7), activation='relu',padding='same', input_shape=(28, 28, 1))) \n","# model.add(MaxPooling2D((2, 2)))                                           \n","# model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))                         \n","# model.add(MaxPooling2D((2, 2)))                                           \n","# model.add(Flatten())                                                      \n","# model.add(Dense(64,activation='relu')) \n","# model.add(Dense(10,activation='softmax'))                               \n","# model.summary()\n","\n","#Droupout y BatchNormalization\n","def make_model():  \n","    model = keras.Sequential()\n","    model.add(Conv2D(filters=32,kernel_size=(3,3),\n","                     activation='relu',strides=1,padding='same',\n","                     input_shape=(28,28,1)))\n","    model.add(BatchNormalization())\n","\n","    model.add(Conv2D(filters=32,kernel_size=(3,3),\n","                     activation='relu',strides=1,padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(filters=64,kernel_size=(3,3),\n","                     activation='relu',strides=1,padding='same'))\n","    model.add(MaxPooling2D(pool_size=(2,2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(filters=128,kernel_size=(3,3),\n","                     activation='relu',strides=1,padding='same'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.25))\n","\n","    model.add(Flatten())\n","    model.add(Dense(512,activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.25))\n","    model.add(Dense(128,activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.25))\n","    model.add(Dense(10,activation='softmax'))\n","    return model\n","\n","model=make_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qA4VID6iySPP","colab_type":"code","colab":{}},"source":["optimizer=tf.keras.optimizers.Adam(lr=0.001) #asignamos una tasa de 0.001\n","\n","model.compile(loss='sparse_categorical_crossentropy', #funcion objetivo\n","               optimizer=optimizer,                 #con el doble de neuronas sgd 87% adam 91%, con neuronas primer ejemplo sgd 84% adam 91%\n","              metrics=['accuracy'])            #metrica precision\n","\n","reduce_rl=tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3*0.9**x) #toma como argumento la funcion de disminucion de pasos \n","                                                                          #devuelve las tasas de aprendizaje actualizadas "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vc95ZGTzyhHe","colab_type":"code","colab":{}},"source":["#entrenamiento\n","model.fit(train_images, train_labels, epochs=10, callbacks=[reduce_rl]) \n","test_loss, test_acc = model.evaluate(test_images, test_labels) \n","print('Test precision:', test_acc)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1SaFKg9T6-F","colab_type":"code","colab":{}},"source":["#predicción usando el modelo\n","predicciones = model.predict(test_images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JuiJ2-PnSrdo","colab_type":"code","colab":{}},"source":["for i in range(20):\n","  print(class_names[test_labels[i]])\n","  print(class_names[np.argmax(predicciones[i])])\n","  print()"],"execution_count":null,"outputs":[]}]}